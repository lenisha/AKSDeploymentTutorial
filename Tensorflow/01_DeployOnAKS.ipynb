{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deploy Web App on Azure Container Services (AKS)\n",
    "In this notebook, we will set up an Azure Container Service which will be managed by Kubernetes. We will then take the Docker image we created earlier that contains our app and deploy it to the AKS cluster. Then, we will check everything is working by sending an image to it and getting it scored.\n",
    "    \n",
    "The process is split into the following steps:\n",
    "* [Define our resource names](#section1)\n",
    "* [Login to Azure](#section2)\n",
    "* [Connect to AKS](#section3)\n",
    "* [Deploy our app](#section5)\n",
    "\n",
    "\n",
    "This guide assumes is designed to be run on linux and requires that the Azure CLI is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Setup\n",
    "Below are the various name definitions for the resources needed to setup ACS as well as the name of the Docker image we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some outputs (and inputs) below have been hidden/masked for confidentiality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please modify the below as you see fit\n",
    "resource_group = \"ml-aks\" \n",
    "aks_name = \"demoaks\"\n",
    "location = \"eastus\"\n",
    "\n",
    "image_name = 'lenisha/tfresnet-gpu' # 'masalvar/tfresnet-gpu' Feel free to use this Image if you want to \n",
    "                                   # skip creating your own container\n",
    "selected_subscription = \"'f869415f-5cff-46a3-b728-20659d14d62d'\" # If you have multiple subscriptions select \n",
    "                                                # the subscription you want to use here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Azure account login\n",
    "The command below will initiate a login to your Azure account. It will pop up with an url to go to where you will enter a one off code and log into your Azure account using your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"cloudName\": \"AzureCloud\",\n",
      "    \"id\": \"f869415f-5cff-46a3-b728-20659d14d62d\",\n",
      "    \"isDefault\": true,\n",
      "    \"name\": \"Microsoft Azure Internal Consumption\",\n",
      "    \"state\": \"Enabled\",\n",
      "    \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
      "    \"user\": {\n",
      "      \"name\": \"http://vstspacker\",\n",
      "      \"type\": \"servicePrincipal\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "{\n",
      "  \"environmentName\": \"AzureCloud\",\n",
      "  \"id\": \"f869415f-5cff-46a3-b728-20659d14d62d\",\n",
      "  \"isDefault\": true,\n",
      "  \"name\": \"Microsoft Azure Internal Consumption\",\n",
      "  \"state\": \"Enabled\",\n",
      "  \"tenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
      "  \"user\": {\n",
      "    \"name\": \"http://vstspacker\",\n",
      "    \"type\": \"servicePrincipal\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!az login --service-principal -u http://vstspacker -p vstspacker123! --tenant 72f988bf-86f1-41af-91ab-2d7cd011db47\n",
    "!az account show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install kubectl CLI\n",
    "\n",
    "To connect to the Kubernetes cluster, we will use kubectl, the Kubernetes command-line client. To install, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading client to /usr/local/bin/kubectl from https://storage.googleapis.com/kubernetes-release/release/v1.10.3/bin/linux/amd64/kubectl\u001b[0m\n",
      "\u001b[33mPlease ensure that /usr/local/bin is in your search PATH, so the `kubectl` command can be found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!az aks install-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Connect to AKS cluster\n",
    "\n",
    "To configure kubectl to connect to the Kubernetes cluster, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged \"demoaks\" as current context in /home/mmlspark/.kube/config\r\n"
     ]
    }
   ],
   "source": [
    "!az aks get-credentials --resource-group $resource_group --name $aks_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify connection by listing the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS    ROLES     AGE       VERSION\r\n",
      "aks-agentpool-23589211-0   Ready     agent     16h       v1.9.6\r\n",
      "aks-agentpool-23589211-1   Ready     agent     1h        v1.9.6\r\n",
      "aks-agentpool-23589211-2   Ready     agent     1h        v1.9.6\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the pods on our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                                              READY     STATUS             RESTARTS   AGE\r\n",
      "default       azure-dl-f568985df-4hjr9                                          1/1       Running            0          23m\r\n",
      "kube-system   addon-http-application-routing-default-http-backend-66c97ffn2hl   1/1       Running            0          16h\r\n",
      "kube-system   addon-http-application-routing-external-dns-8d5bf7cd7-jkmmf       1/1       Running            0          16h\r\n",
      "kube-system   addon-http-application-routing-nginx-ingress-controller-64888pj   1/1       Running            0          16h\r\n",
      "kube-system   heapster-7cd8dd888b-rx8sl                                         1/2       CrashLoopBackOff   194        16h\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-bxv6z                                     3/3       Running            0          16h\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-hbbpr                                     3/3       Running            0          16h\r\n",
      "kube-system   kube-proxy-gp29g                                                  1/1       Running            0          1h\r\n",
      "kube-system   kube-proxy-k8bxt                                                  1/1       Running            0          1h\r\n",
      "kube-system   kube-proxy-klxh8                                                  1/1       Running            0          16h\r\n",
      "kube-system   kube-svc-redirect-bzb9b                                           1/1       Running            0          1h\r\n",
      "kube-system   kube-svc-redirect-pd4hw                                           1/1       Running            0          16h\r\n",
      "kube-system   kube-svc-redirect-xg2sf                                           1/1       Running            0          1h\r\n",
      "kube-system   kubernetes-dashboard-546f987686-ld5cl                             1/1       Running            0          16h\r\n",
      "kube-system   omsagent-6xprt                                                    1/1       Running            0          1h\r\n",
      "kube-system   omsagent-q2jzq                                                    1/1       Running            1          16h\r\n",
      "kube-system   omsagent-tm5q4                                                    1/1       Running            0          1h\r\n",
      "kube-system   tunnelfront-68b6cbbb96-s44pc                                      1/1       Running            0          16h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Deploy application\n",
    "\n",
    "Below we define our Kubernetes manifest file for our service and load balancer. Note that we have to specify the volume mounts to the drivers that are located on the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_template = {\n",
    "    \"apiVersion\": \"apps/v1\",\n",
    "    \"kind\": \"Deployment\",\n",
    "    \"metadata\": {\n",
    "        \"name\": \"azure-dl\"\n",
    "    },\n",
    "    \"spec\": {\n",
    "        \"replicas\": 1,\n",
    "        \"selector\": {\n",
    "          \"matchLabels\": {\"app\": \"azure-dl\"}\n",
    "          \n",
    "        },\n",
    "        \"strategy\": {\n",
    "          \"type\": \"RollingUpdate\",\n",
    "          \"rollingUpdate\": {\n",
    "            \"maxSurge\": 1,\n",
    "            \"maxUnavailable\": 1\n",
    "          }\n",
    "        },\n",
    "        \"minReadySeconds\": 15,\n",
    "\n",
    "        \"template\": {\n",
    "            \"metadata\": {\n",
    "                \"labels\": {\n",
    "                    \"app\": \"azure-dl\"\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"containers\": [\n",
    "                    {\n",
    "                        \"env\": [\n",
    "                            {\n",
    "                                \"name\": \"LD_LIBRARY_PATH\",\n",
    "                                \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.6/lib\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"image\": image_name,\n",
    "                        \n",
    "                        \"name\": \"azure-dl\",\n",
    "                       \n",
    "                        \n",
    "                        \"ports\": [\n",
    "                            {\n",
    "                                \"containerPort\": 80,\n",
    "                                \"name\": \"model\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"resources\": {\n",
    "                            \"limits\": {\n",
    "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                            },\n",
    "                            \"requests\": {\n",
    "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\n",
    "                            }\n",
    "                        },\n",
    "                        \"volumeMounts\": [\n",
    "                            {\n",
    "                                \"mountPath\": \"/usr/local/nvidia\",\n",
    "                                \"name\": \"nvidia\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"volumes\": [\n",
    "                    {\n",
    "                        \"hostPath\": {\n",
    "                            \"path\": \"/usr/local/nvidia\"\n",
    "                        },\n",
    "                        \"name\": \"nvidia\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "service_temp = {\n",
    "  \n",
    "    \"apiVersion\": \"v1\",\n",
    "    \"kind\": \"Service\",\n",
    "    \"metadata\": {\n",
    "        \"name\": \"azure-dl\"\n",
    "    },\n",
    "    \"spec\": {\n",
    "        \"ports\": [\n",
    "            {\n",
    "                \"port\": 80\n",
    "            }\n",
    "        ],\n",
    "        \"selector\": {\n",
    "            \"app\": \"azure-dl\"\n",
    "        },\n",
    "        \"type\": \"LoadBalancer\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_json_to_file(json_dict, filename, mode='w'):\n",
    "    with open(filename, mode) as outfile:\n",
    "        json.dump(json_dict, outfile, indent=4,sort_keys=True)\n",
    "        outfile.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(app_template, 'az-dl.json') # We write the service template to the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(service_temp, 'az-dl.json', mode='a') # We add the loadbelanacer template to the json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the manifest created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"apiVersion\": \"apps/v1\",\r\n",
      "    \"kind\": \"Deployment\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-dl\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"minReadySeconds\": 15,\r\n",
      "        \"replicas\": 1,\r\n",
      "        \"selector\": {\r\n",
      "            \"matchLabels\": {\r\n",
      "                \"app\": \"azure-dl\"\r\n",
      "            }\r\n",
      "        },\r\n",
      "        \"strategy\": {\r\n",
      "            \"rollingUpdate\": {\r\n",
      "                \"maxSurge\": 1,\r\n",
      "                \"maxUnavailable\": 1\r\n",
      "            },\r\n",
      "            \"type\": \"RollingUpdate\"\r\n",
      "        },\r\n",
      "        \"template\": {\r\n",
      "            \"metadata\": {\r\n",
      "                \"labels\": {\r\n",
      "                    \"app\": \"azure-dl\"\r\n",
      "                }\r\n",
      "            },\r\n",
      "            \"spec\": {\r\n",
      "                \"containers\": [\r\n",
      "                    {\r\n",
      "                        \"env\": [\r\n",
      "                            {\r\n",
      "                                \"name\": \"LD_LIBRARY_PATH\",\r\n",
      "                                \"value\": \"$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/opt/conda/envs/py3.6/lib\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"image\": \"lenisha/tfresnet-gpu\",\r\n",
      "                        \"name\": \"azure-dl\",\r\n",
      "                        \"ports\": [\r\n",
      "                            {\r\n",
      "                                \"containerPort\": 80,\r\n",
      "                                \"name\": \"model\"\r\n",
      "                            }\r\n",
      "                        ],\r\n",
      "                        \"resources\": {\r\n",
      "                            \"limits\": {\r\n",
      "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\r\n",
      "                            },\r\n",
      "                            \"requests\": {\r\n",
      "                                \"alpha.kubernetes.io/nvidia-gpu\": 1\r\n",
      "                            }\r\n",
      "                        },\r\n",
      "                        \"volumeMounts\": [\r\n",
      "                            {\r\n",
      "                                \"mountPath\": \"/usr/local/nvidia\",\r\n",
      "                                \"name\": \"nvidia\"\r\n",
      "                            }\r\n",
      "                        ]\r\n",
      "                    }\r\n",
      "                ],\r\n",
      "                \"volumes\": [\r\n",
      "                    {\r\n",
      "                        \"hostPath\": {\r\n",
      "                            \"path\": \"/usr/local/nvidia\"\r\n",
      "                        },\r\n",
      "                        \"name\": \"nvidia\"\r\n",
      "                    }\r\n",
      "                ]\r\n",
      "            }\r\n",
      "        }\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n",
      "{\r\n",
      "    \"apiVersion\": \"v1\",\r\n",
      "    \"kind\": \"Service\",\r\n",
      "    \"metadata\": {\r\n",
      "        \"name\": \"azure-dl\"\r\n",
      "    },\r\n",
      "    \"spec\": {\r\n",
      "        \"ports\": [\r\n",
      "            {\r\n",
      "                \"port\": 80\r\n",
      "            }\r\n",
      "        ],\r\n",
      "        \"selector\": {\r\n",
      "            \"app\": \"azure-dl\"\r\n",
      "        },\r\n",
      "        \"type\": \"LoadBalancer\"\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat az-dl.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use kubectl create command to deploy our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f az-dl.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the pod is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                                              READY     STATUS             RESTARTS   AGE\r\n",
      "default       azure-dl-5859fc8467-hk9gx                                         1/1       Running            0          1h\r\n",
      "default       azure-dl-5859fc8467-pcxt5                                         1/1       Running            0          35s\r\n",
      "default       azure-dl-5859fc8467-sgdbn                                         1/1       Running            0          35s\r\n",
      "kube-system   addon-http-application-routing-default-http-backend-66c97ffn2hl   1/1       Running            0          15h\r\n",
      "kube-system   addon-http-application-routing-external-dns-8d5bf7cd7-jkmmf       1/1       Running            0          15h\r\n",
      "kube-system   addon-http-application-routing-nginx-ingress-controller-64888pj   1/1       Running            0          15h\r\n",
      "kube-system   heapster-7cd8dd888b-rx8sl                                         1/2       CrashLoopBackOff   183        15h\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-bxv6z                                     3/3       Running            0          15h\r\n",
      "kube-system   kube-dns-v20-7c556f89c5-hbbpr                                     3/3       Running            0          15h\r\n",
      "kube-system   kube-proxy-gp29g                                                  1/1       Running            0          50m\r\n",
      "kube-system   kube-proxy-k8bxt                                                  1/1       Running            0          50m\r\n",
      "kube-system   kube-proxy-klxh8                                                  1/1       Running            0          15h\r\n",
      "kube-system   kube-svc-redirect-bzb9b                                           1/1       Running            0          50m\r\n",
      "kube-system   kube-svc-redirect-pd4hw                                           1/1       Running            0          15h\r\n",
      "kube-system   kube-svc-redirect-xg2sf                                           1/1       Running            0          50m\r\n",
      "kube-system   kubernetes-dashboard-546f987686-ld5cl                             1/1       Running            0          15h\r\n",
      "kube-system   omsagent-6xprt                                                    1/1       Running            0          50m\r\n",
      "kube-system   omsagent-q2jzq                                                    1/1       Running            1          15h\r\n",
      "kube-system   omsagent-tm5q4                                                    1/1       Running            0          50m\r\n",
      "kube-system   tunnelfront-68b6cbbb96-s44pc                                      1/1       Running            0          15h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything goes wrong you can use the commands below to observe the events on the node as well as review the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST SEEN   FIRST SEEN   COUNT     NAME                                         KIND                      SUBOBJECT                   TYPE      REASON                         SOURCE                                 MESSAGE\r\n",
      "50m         50m          1         aks-agentpool-23589211-1.15329f359402b61b    Node                                                  Normal    Starting                       kubelet, aks-agentpool-23589211-1      Starting kubelet.\r\n",
      "50m         50m          2         aks-agentpool-23589211-1.15329f35986ec240    Node                                                  Normal    NodeHasSufficientDisk          kubelet, aks-agentpool-23589211-1      Node aks-agentpool-23589211-1 status is now: NodeHasSufficientDisk\r\n",
      "50m         50m          2         aks-agentpool-23589211-1.15329f35986ef7c4    Node                                                  Normal    NodeHasSufficientMemory        kubelet, aks-agentpool-23589211-1      Node aks-agentpool-23589211-1 status is now: NodeHasSufficientMemory\r\n",
      "50m         50m          2         aks-agentpool-23589211-1.15329f35986f2064    Node                                                  Normal    NodeHasNoDiskPressure          kubelet, aks-agentpool-23589211-1      Node aks-agentpool-23589211-1 status is now: NodeHasNoDiskPressure\r\n",
      "50m         50m          1         aks-agentpool-23589211-1.15329f3599409a30    Node                                                  Normal    NodeAllocatableEnforced        kubelet, aks-agentpool-23589211-1      Updated Node Allocatable limit across pods\r\n",
      "50m         50m          1         aks-agentpool-23589211-1.15329f35c2ed16a1    Node                                                  Normal    RegisteredNode                 node-controller                        Node aks-agentpool-23589211-1 event: Registered Node aks-agentpool-23589211-1 in Controller\r\n",
      "50m         50m          1         aks-agentpool-23589211-1.15329f3a4bc323d0    Node                                                  Normal    NodeReady                      kubelet, aks-agentpool-23589211-1      Node aks-agentpool-23589211-1 status is now: NodeReady\r\n",
      "50m         50m          1         aks-agentpool-23589211-1.15329f3d856c8650    Node                                                  Normal    Starting                       kube-proxy, aks-agentpool-23589211-1   Starting kube-proxy.\r\n",
      "50m         50m          1         aks-agentpool-23589211-2.15329f3367b7a707    Node                                                  Normal    Starting                       kubelet, aks-agentpool-23589211-2      Starting kubelet.\r\n",
      "50m         50m          2         aks-agentpool-23589211-2.15329f336c05a5f2    Node                                                  Normal    NodeHasSufficientDisk          kubelet, aks-agentpool-23589211-2      Node aks-agentpool-23589211-2 status is now: NodeHasSufficientDisk\r\n",
      "50m         50m          2         aks-agentpool-23589211-2.15329f336c05d4d2    Node                                                  Normal    NodeHasSufficientMemory        kubelet, aks-agentpool-23589211-2      Node aks-agentpool-23589211-2 status is now: NodeHasSufficientMemory\r\n",
      "50m         50m          2         aks-agentpool-23589211-2.15329f336c060d75    Node                                                  Normal    NodeHasNoDiskPressure          kubelet, aks-agentpool-23589211-2      Node aks-agentpool-23589211-2 status is now: NodeHasNoDiskPressure\r\n",
      "50m         50m          1         aks-agentpool-23589211-2.15329f336ce1508f    Node                                                  Normal    NodeAllocatableEnforced        kubelet, aks-agentpool-23589211-2      Updated Node Allocatable limit across pods\r\n",
      "50m         50m          1         aks-agentpool-23589211-2.15329f3498e17677    Node                                                  Normal    RegisteredNode                 node-controller                        Node aks-agentpool-23589211-2 event: Registered Node aks-agentpool-23589211-2 in Controller\r\n",
      "50m         50m          1         aks-agentpool-23589211-2.15329f36e785a0bd    Node                                                  Normal    Starting                       kube-proxy, aks-agentpool-23589211-2   Starting kube-proxy.\r\n",
      "50m         50m          1         aks-agentpool-23589211-2.15329f381f988050    Node                                                  Normal    NodeReady                      kubelet, aks-agentpool-23589211-2      Node aks-agentpool-23589211-2 status is now: NodeReady\r\n",
      "48s         48s          1         azure-dl-5859fc8467-pcxt5.1532a1eda664d8e5   Pod                                                   Normal    Scheduled                      default-scheduler                      Successfully assigned azure-dl-5859fc8467-pcxt5 to aks-agentpool-23589211-2\r\n",
      "48s         48s          1         azure-dl-5859fc8467-pcxt5.1532a1edb4b5c05d   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-2      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "48s         48s          1         azure-dl-5859fc8467-pcxt5.1532a1edb575e989   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-2      MountVolume.SetUp succeeded for volume \"default-token-5v5xd\" \r\n",
      "47s         47s          1         azure-dl-5859fc8467-pcxt5.1532a1eddd60931a   Pod                       spec.containers{azure-dl}   Normal    Pulling                        kubelet, aks-agentpool-23589211-2      pulling image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "44s         44s          1         azure-dl-5859fc8467-pcxt5.1532a1ee76ea0e5f   Pod                       spec.containers{azure-dl}   Normal    Pulled                         kubelet, aks-agentpool-23589211-2      Successfully pulled image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "44s         44s          1         azure-dl-5859fc8467-pcxt5.1532a1ee83ab95be   Pod                       spec.containers{azure-dl}   Normal    Created                        kubelet, aks-agentpool-23589211-2      Created container\r\n",
      "44s         44s          1         azure-dl-5859fc8467-pcxt5.1532a1ee8b2d411c   Pod                       spec.containers{azure-dl}   Normal    Started                        kubelet, aks-agentpool-23589211-2      Started container\r\n",
      "51m         54m          15        azure-dl-5859fc8467-rjwzf.15329efecf613a5d   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "50m         50m          1         azure-dl-5859fc8467-rjwzf.15329f381ebeef95   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu, 2 NodeNotReady.\r\n",
      "50m         50m          1         azure-dl-5859fc8467-rjwzf.15329f38205e8618   Pod                                                   Normal    Scheduled                      default-scheduler                      Successfully assigned azure-dl-5859fc8467-rjwzf to aks-agentpool-23589211-2\r\n",
      "50m         50m          1         azure-dl-5859fc8467-rjwzf.15329f3827b103f7   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-2      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "50m         50m          1         azure-dl-5859fc8467-rjwzf.15329f38284ab3f5   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-2      MountVolume.SetUp succeeded for volume \"default-token-5v5xd\" \r\n",
      "50m         50m          1         azure-dl-5859fc8467-rjwzf.15329f385dc71d29   Pod                       spec.containers{azure-dl}   Normal    Pulling                        kubelet, aks-agentpool-23589211-2      pulling image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "47m         47m          1         azure-dl-5859fc8467-rjwzf.15329f62f714f002   Pod                       spec.containers{azure-dl}   Normal    Pulled                         kubelet, aks-agentpool-23589211-2      Successfully pulled image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "47m         47m          1         azure-dl-5859fc8467-rjwzf.15329f63021bf4b1   Pod                       spec.containers{azure-dl}   Normal    Created                        kubelet, aks-agentpool-23589211-2      Created container\r\n",
      "47m         47m          1         azure-dl-5859fc8467-rjwzf.15329f630c82e33c   Pod                       spec.containers{azure-dl}   Normal    Started                        kubelet, aks-agentpool-23589211-2      Started container\r\n",
      "54s         54s          1         azure-dl-5859fc8467-rjwzf.1532a1ec3f72f1cf   Pod                       spec.containers{azure-dl}   Normal    Killing                        kubelet, aks-agentpool-23589211-2      Killing container with id docker://azure-dl:Need to kill Pod\r\n",
      "41s         48s          5         azure-dl-5859fc8467-sgdbn.1532a1eda66ecafa   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 3 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "33s         33s          1         azure-dl-5859fc8467-sgdbn.1532a1f126445217   Pod                                                   Normal    Scheduled                      default-scheduler                      Successfully assigned azure-dl-5859fc8467-sgdbn to aks-agentpool-23589211-1\r\n",
      "33s         33s          1         azure-dl-5859fc8467-sgdbn.1532a1f134adec37   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-1      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "33s         33s          1         azure-dl-5859fc8467-sgdbn.1532a1f1359a4641   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-1      MountVolume.SetUp succeeded for volume \"default-token-5v5xd\" \r\n",
      "32s         32s          1         azure-dl-5859fc8467-sgdbn.1532a1f15e19a1a9   Pod                       spec.containers{azure-dl}   Normal    Pulling                        kubelet, aks-agentpool-23589211-1      pulling image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "30s         30s          1         azure-dl-5859fc8467-sgdbn.1532a1f1c05f437f   Pod                       spec.containers{azure-dl}   Normal    Pulled                         kubelet, aks-agentpool-23589211-1      Successfully pulled image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "30s         30s          1         azure-dl-5859fc8467-sgdbn.1532a1f1cd89cb2d   Pod                       spec.containers{azure-dl}   Normal    Created                        kubelet, aks-agentpool-23589211-1      Created container\r\n",
      "30s         30s          1         azure-dl-5859fc8467-sgdbn.1532a1f1d6f051c0   Pod                       spec.containers{azure-dl}   Normal    Started                        kubelet, aks-agentpool-23589211-1      Started container\r\n",
      "51m         54m          15        azure-dl-5859fc8467-sj22w.15329efed021f402   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "50m         50m          2         azure-dl-5859fc8467-sj22w.15329f381b0dbffc   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu, 2 NodeNotReady.\r\n",
      "50m         50m          4         azure-dl-5859fc8467-sj22w.15329f38579aa84d   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 NodeNotReady, 2 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "50m         50m          1         azure-dl-5859fc8467-sj22w.15329f3b9aeb38a0   Pod                                                   Normal    Scheduled                      default-scheduler                      Successfully assigned azure-dl-5859fc8467-sj22w to aks-agentpool-23589211-1\r\n",
      "50m         50m          1         azure-dl-5859fc8467-sj22w.15329f3ba81d0707   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-1      MountVolume.SetUp succeeded for volume \"nvidia\" \r\n",
      "50m         50m          1         azure-dl-5859fc8467-sj22w.15329f3baa53c7c0   Pod                                                   Normal    SuccessfulMountVolume          kubelet, aks-agentpool-23589211-1      MountVolume.SetUp succeeded for volume \"default-token-5v5xd\" \r\n",
      "47m         50m          2         azure-dl-5859fc8467-sj22w.15329f3d720186cf   Pod                       spec.containers{azure-dl}   Normal    Pulling                        kubelet, aks-agentpool-23589211-1      pulling image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "48m         48m          1         azure-dl-5859fc8467-sj22w.15329f58071886e3   Pod                       spec.containers{azure-dl}   Warning   Failed                         kubelet, aks-agentpool-23589211-1      Failed to pull image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\": rpc error: code = Canceled desc = context canceled\r\n",
      "48m         48m          1         azure-dl-5859fc8467-sj22w.15329f580718dec7   Pod                       spec.containers{azure-dl}   Warning   Failed                         kubelet, aks-agentpool-23589211-1      Error: ErrImagePull\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48m         48m          1         azure-dl-5859fc8467-sj22w.15329f5822f9107d   Pod                                                   Normal    SandboxChanged                 kubelet, aks-agentpool-23589211-1      Pod sandbox changed, it will be killed and re-created.\r\n",
      "44m         44m          1         azure-dl-5859fc8467-sj22w.15329f87b850191f   Pod                       spec.containers{azure-dl}   Normal    Pulled                         kubelet, aks-agentpool-23589211-1      Successfully pulled image \"mldemoacr.azurecr.io/ml/tfresnet-gpu\"\r\n",
      "44m         44m          1         azure-dl-5859fc8467-sj22w.15329f87c4fb8697   Pod                       spec.containers{azure-dl}   Normal    Created                        kubelet, aks-agentpool-23589211-1      Created container\r\n",
      "44m         44m          1         azure-dl-5859fc8467-sj22w.15329f87ccb9b575   Pod                       spec.containers{azure-dl}   Normal    Started                        kubelet, aks-agentpool-23589211-1      Started container\r\n",
      "43s         43s          1         azure-dl-5859fc8467-sj22w.1532a1eeb13934e8   Pod                       spec.containers{azure-dl}   Normal    Killing                        kubelet, aks-agentpool-23589211-1      Killing container with id docker://azure-dl:Need to kill Pod\r\n",
      "51m         54m          15        azure-dl-5859fc8467-v7sz5.15329efecfc0b9e6   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "50m         50m          1         azure-dl-5859fc8467-v7sz5.15329f381f2a8c48   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu, 2 NodeNotReady.\r\n",
      "50m         50m          5         azure-dl-5859fc8467-v7sz5.15329f381fe3c02a   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 NodeNotReady, 2 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "34m         50m          54        azure-dl-5859fc8467-v7sz5.15329f3ba06f4349   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 3 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "29m         29m          1         azure-dl-5859fc8467-v7sz5.1532a05d3ba26c14   Pod                                                   Warning   FailedScheduling               default-scheduler                      skip schedule deleting pod: default/azure-dl-5859fc8467-v7sz5\r\n",
      "51m         54m          15        azure-dl-5859fc8467-xw2bt.15329efecee6998d   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/1 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "50m         50m          1         azure-dl-5859fc8467-xw2bt.15329f381f57563e   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 Insufficient alpha.kubernetes.io/nvidia-gpu, 2 NodeNotReady.\r\n",
      "50m         50m          5         azure-dl-5859fc8467-xw2bt.15329f382058060a   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 1 NodeNotReady, 2 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "34m         50m          54        azure-dl-5859fc8467-xw2bt.15329f3ba10cd0b0   Pod                                                   Warning   FailedScheduling               default-scheduler                      0/3 nodes are available: 3 Insufficient alpha.kubernetes.io/nvidia-gpu.\r\n",
      "29m         29m          1         azure-dl-5859fc8467-xw2bt.1532a05d3bb33298   Pod                                                   Warning   FailedScheduling               default-scheduler                      skip schedule deleting pod: default/azure-dl-5859fc8467-xw2bt\r\n",
      "54m         54m          1         azure-dl-5859fc8467.15329efecef286d0         ReplicaSet                                            Normal    SuccessfulCreate               replicaset-controller                  Created pod: azure-dl-5859fc8467-xw2bt\r\n",
      "54m         54m          1         azure-dl-5859fc8467.15329efecf547842         ReplicaSet                                            Normal    SuccessfulCreate               replicaset-controller                  Created pod: azure-dl-5859fc8467-rjwzf\r\n",
      "54m         54m          1         azure-dl-5859fc8467.15329efecf604293         ReplicaSet                                            Normal    SuccessfulCreate               replicaset-controller                  Created pod: azure-dl-5859fc8467-v7sz5\r\n",
      "54m         54m          1         azure-dl-5859fc8467.15329efecfe525f9         ReplicaSet                                            Normal    SuccessfulCreate               replicaset-controller                  Created pod: azure-dl-5859fc8467-sj22w\r\n",
      "29m         29m          1         azure-dl-5859fc8467.1532a05d3c7cde52         ReplicaSet                                            Normal    SuccessfulDelete               replicaset-controller                  Deleted pod: azure-dl-5859fc8467-xw2bt\r\n",
      "29m         29m          1         azure-dl-5859fc8467.1532a05d3cc2f75c         ReplicaSet                                            Normal    SuccessfulDelete               replicaset-controller                  Deleted pod: azure-dl-5859fc8467-v7sz5\r\n",
      "55s         55s          1         azure-dl-5859fc8467.1532a1ec0b5eed66         ReplicaSet                                            Normal    SuccessfulDelete               replicaset-controller                  Deleted pod: azure-dl-5859fc8467-sj22w\r\n",
      "55s         55s          1         azure-dl-5859fc8467.1532a1ec0b61b903         ReplicaSet                                            Normal    SuccessfulDelete               replicaset-controller                  Deleted pod: azure-dl-5859fc8467-rjwzf\r\n",
      "48s         48s          1         azure-dl-5859fc8467.1532a1eda60d50d1         ReplicaSet                                            Normal    SuccessfulCreate               replicaset-controller                  Created pod: azure-dl-5859fc8467-pcxt5\r\n",
      "48s         48s          1         azure-dl-5859fc8467.1532a1eda6721fd9         ReplicaSet                                            Normal    SuccessfulCreate               replicaset-controller                  Created pod: azure-dl-5859fc8467-sgdbn\r\n",
      "55s         1h           2         azure-dl.15329e10fa18b516                    Service                                               Normal    EnsuringLoadBalancer           service-controller                     Ensuring load balancer\r\n",
      "52s         1h           2         azure-dl.15329e1e89ac6ee7                    Service                                               Normal    EnsuredLoadBalancer            service-controller                     Ensured load balancer\r\n",
      "54m         54m          1         azure-dl.15329efecb4243be                    Deployment                                            Normal    ScalingReplicaSet              deployment-controller                  Scaled up replica set azure-dl-5859fc8467 to 5\r\n",
      "48m         48m          1         azure-dl.15329f5035af83b6                    Service                                               Normal    UpdatedLoadBalancer            service-controller                     Updated load balancer with new hosts\r\n",
      "6m          36m          61        azure-dl.15329ffd8839c016                    HorizontalPodAutoscaler                               Warning   FailedGetResourceMetric        horizontal-pod-autoscaler              unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)\r\n",
      "1m          36m          71        azure-dl.15329ffd887385b4                    HorizontalPodAutoscaler                               Warning   FailedComputeMetricsReplicas   horizontal-pod-autoscaler              failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)\r\n",
      "29m         29m          1         azure-dl.1532a05d3947b72b                    Deployment                                            Normal    ScalingReplicaSet              deployment-controller                  Scaled down replica set azure-dl-5859fc8467 to 3\r\n",
      "55s         55s          1         azure-dl.1532a1ec09f0c6e2                    Deployment                                            Normal    ScalingReplicaSet              deployment-controller                  Scaled down replica set azure-dl-5859fc8467 to 1\r\n",
      "48s         48s          1         azure-dl.1532a1eda58092e5                    Deployment                                            Normal    ScalingReplicaSet              deployment-controller                  Scaled up replica set azure-dl-5859fc8467 to 3\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-27 21:37:04,421 CRIT Supervisor running as root (no user in config file)\r\n",
      "2018-05-27 21:37:04,423 INFO supervisord started with pid 1\r\n",
      "2018-05-27 21:37:05,425 INFO spawned: 'program_exit' with pid 10\r\n",
      "2018-05-27 21:37:05,427 INFO spawned: 'nginx' with pid 11\r\n",
      "2018-05-27 21:37:05,428 INFO spawned: 'gunicorn' with pid 12\r\n",
      "2018-05-27 21:37:06,460 INFO success: program_exit entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\r\n",
      "2018-05-27 21:37:07.000605: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "2018-05-27 21:37:07.194163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n",
      "pciBusID: c0fb:00:00.0\r\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\r\n",
      "2018-05-27 21:37:07.194207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: c0fb:00:00.0, compute capability: 3.7)\r\n",
      "2018-05-27 21:37:11,199 INFO success: nginx entered RUNNING state, process has stayed up for > than 5 seconds (startsecs)\r\n",
      "INFO:tensorflow:Restoring parameters from resnet_v1_152.ckpt\r\n",
      "{\"tags\": [], \"msg\": \"Restoring parameters from %s\", \"message\": \"Restoring parameters from resnet_v1_152.ckpt\", \"host\": \"azure-dl-5859fc8467-hk9gx\", \"logger\": \"tensorflow\", \"timestamp\": \"2018-05-27T21:37:12.546813Z\", \"stack_info\": null, \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/tensorflow/python/platform/tf_logging.py\", \"level\": \"INFO\"}\r\n",
      "{\"tags\": [], \"message\": \"Model loading time: 7398.26 ms\", \"host\": \"azure-dl-5859fc8467-hk9gx\", \"logger\": \"model_driver\", \"timestamp\": \"2018-05-27T21:37:14.397617Z\", \"stack_info\": null, \"path\": \"/code/driver.py\", \"level\": \"INFO\"}\r\n",
      "Initialising\r\n",
      "{\"tags\": [], \"msg\": \" * Running on %s://%s:%d/ %s\", \"message\": \" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\", \"host\": \"azure-dl-5859fc8467-hk9gx\", \"logger\": \"werkzeug\", \"timestamp\": \"2018-05-27T21:37:14.403732Z\", \"stack_info\": null, \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\", \"level\": \"INFO\"}\r\n",
      "2018-05-27 21:37:26,418 INFO success: gunicorn entered RUNNING state, process has stayed up for > than 20 seconds (startsecs)\r\n",
      "{\"tags\": [], \"message\": \"127.0.0.1 - - [27/May/2018 22:16:58] \\\"GET / HTTP/1.0\\\" 200 -\", \"host\": \"azure-dl-5859fc8467-hk9gx\", \"logger\": \"werkzeug\", \"timestamp\": \"2018-05-27T22:16:58.830592Z\", \"stack_info\": null, \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\", \"level\": \"INFO\"}\r\n",
      "{\"tags\": [], \"message\": \"127.0.0.1 - - [27/May/2018 22:20:07] \\\"GET /version HTTP/1.0\\\" 200 -\", \"host\": \"azure-dl-5859fc8467-hk9gx\", \"logger\": \"werkzeug\", \"timestamp\": \"2018-05-27T22:20:07.251976Z\", \"stack_info\": null, \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\", \"level\": \"INFO\"}\r\n",
      "{\"tags\": [], \"message\": \"127.0.0.1 - - [27/May/2018 22:20:23] \\\"GET /score.py HTTP/1.0\\\" 404 -\", \"host\": \"azure-dl-5859fc8467-hk9gx\", \"logger\": \"werkzeug\", \"timestamp\": \"2018-05-27T22:20:23.098279Z\", \"stack_info\": null, \"path\": \"/opt/conda/envs/py3.5/lib/python3.5/site-packages/werkzeug/_internal.py\", \"level\": \"INFO\"}\r\n"
     ]
    }
   ],
   "source": [
    "pod_json = !kubectl get pods -o json\n",
    "pod_dict = json.loads(''.join(pod_json))\n",
    "!kubectl logs {pod_dict['items'][0]['metadata']['name']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a few minutes for the service to populate the EXTERNAL-IP field. This will be the IP you use to call the service. You can also specify an IP to use please see the AKS documentation for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE\r\n",
      "azure-dl   LoadBalancer   10.0.1.67    104.211.53.76   80:32186/TCP   1h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service azure-dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our deployed service we can move onto [testing it](02_TestWebApp.ipynb)  \n",
    "Below are the instructions to tear everything down once we are done with the cluster"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Tensorflow remotedocker-gpu",
   "language": "python",
   "name": "tensorflow_remotedocker-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
